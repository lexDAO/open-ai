 So without much further ado, if people in the area can quiet down. Just to kind of introduce why I'm up here on the exploit panel. So throughout my career, I was a general counsel of a stock exchange. I sort of got thrown into cybersecurity when I ended up becoming effectively the CISO of a stock exchange. Due to a problem interpreting a decimal point, we, the exchange accumulated about a billion dollars in trades that never happened. So that was a bit of a problem with the SEC, so I had to get much more involved. And then subsequent to all that, because there was a lot of introspection, I started, I opened a cybersecurity company, which I ran for about five years, boarded company in Bangladesh, built that whole software suite. And you know, my exit was the multiple of, but anyway, very successful, not really, but I did learn a lot in the process. In my time as a cybersecurity, I formed a risk assessment company. We did penetration testing, vulnerability scans, cloud assessments throughout. As I extended it into digital assets, one of my clients, I wasn't as involved in security, but one of my clients experienced a roughly a hundred million dollar hack. And so I was heavily involved in the processes surrounding that. And you know, I continued to do different cybersecurity advisory for clients from time to time, although that's not really my main focus anymore. But with that, I think on this panel, we're going to try to cover a lot. We're going to try to cover, you know, how does cybersecurity potentially open us up to the expansion of the administrative state into what we do. But also one of the things I'd like to do is kind of address the fact that, you know, we're all risk managers for our clients. And sometimes we need to step in, even as counsel, to say, hey, here are some things that I think you should do because they're not being addressed. So we're going to talk about incidents and incident responses. But I think I'm just going to lead off with one thing and we'll back into a bunch of other things. But when we talk about like whether it's an exploitation, a hack or what have you, these are all extremely stressful situations. And one of the most critical things is to actually have a plan of action for how you're going to be able to manage it. Now, for a mature organization, this may be, you know, understood. Like, you know, the more mature organizations are going to have a information security function and be able to handle incidents. But for, I'd say the vast majority, it's like, what the fuck do we do now? And so what I see is like, you know, there's not enough attention being given to security and how you handle these exploits. And part of the problem is that these companies, they start up, they're in growth mode. They're trying to build. They're not trying to fix. They're trying to grow their market share aggressively. And security might be just something that they have to do, they're not particularly experienced at, and they leave to their dev teams, which may handle it depending on their experience. But just to kind of launch into one thing that I think every project can and should do right out the gate is due, and this is going to sound crazy in crypto, a tabletop exercise. So a tabletop exercise is something that we used to do in TrabFi space, where you get the key players and they sit around a table and they basically walk through what happens in the event of various exploits. And they use that to create an incident response plan. So you know, who is responsible for what when it happens? Now, in a decentralized protocol, this may be an extra, this exercise itself might be very complicated, right? Like, well, you know, we're decentralized, so it's not really clear who would do that. But I guarantee you, if you've got a problem, you're going to find out who's actually going to be the one managing that incident. And it may or may not be the person who's best suited to do so. So by doing the uncomfortable exercise, the difficult exercise of trying to identify the who, which is, you know, very much, you know, if you've read like the Medilex paper, you know, about the importance sometimes of the human in the loop, time and time and time again, when we see these exploits, when he sees these incidents, the human in the loop becomes critical and all this other shit gets thrown out the window, right? It's like, how do I handle this problem? And it's usually ill-prepared for and it's doing it on the fly. And so just to extend, because I'm trying to give these nuggets, you know, from my experience, and then I do want to kind of, you know, turn it over and look at SEAL and what Gage has been doing there and kind of riff off of that. But you know, time and time again, you have to have the accountability of who. You have to understand, you know, also you need to have some of your relationships established. And when I say you, counselor, you're clients, they need to think who am I going to turn to? Like the SEAL protocol is like an example. That's a resource for projects to turn to. They may have not never even considered who they might turn to in the event they need a WhiteHack service or what have you, but now they have a phone number. Now the availability of that because it's become so popular may be a different question. I mean SEAL's been sort of like, just there's been a lot of inquiries from what I understand and Gabe can talk more about that, but you know, maybe too much because it's been largely voluntary to think about staffing it, et cetera, and somebody has to pay for it. But you know, there's always a stage when a project starts off, first it's just built. Let's get just clients. And nobody even wants to talk about security. Nobody wants to talk about exploit. I'm trying to just like get this thing off the ground. I don't have time and money and mind share to do it. But that can't extend indefinitely or else there's serious risk. At a certain point as a counsel, you got to say, listen, this is a serious risk for your protocol. You got to do some basic things. You got to think about how you're going to handle incidents. And you also got to think about who you're going to call because when a shit hits the fan, you can't be sitting there saying, I don't know who to call. Who's in charge? What am I supposed to do? What's my communication plan? All these things, now you've got like you're extremely compressed for time to kind of come up at these things and time is money. Time could be your protocol and it's critical that you have an execution plan to put into place. And before I, you know, I'll say that even in the case where like we had the almost $1 billion in notional value, I fucking put a procedure list like I emblazoned it on the wall. I had everybody I had read. I had notes and everything else. You know, like a few months later, something happened and like the people were saying, yeah, we didn't know what to do. I'm like, are you freaking kidding me? It's on the fucking wall. Like read the wall. Like it's all over the place. We trade you guys. So my point being is that even with the best efforts, it can go awry. So even if you have an incident response plan, it doesn't really mean anything if nobody in the organization knows that it exists. So I've rambled on for a bit. So maybe a little different direction. But that's all really good stuff. I mean, very similar to anything I would want to say as well. You mentioned seal a few times, but I don't know if you actually explained what it is. It's essentially it was fan. I'm sure you guys all know who Sam Sun is. He sort of, I don't even know if that's correct pronunciation, but he has an anime PFP and he's probably the most famous, you know, smart contract white hat in the space. Right. And now he works for Paradigm. He's done many, you know, multi-multi-million-dollar rescues of funds from vulnerable smart contracts and the type of dude who can just look at a smart contract for like 30 seconds and spot, you know, five vulnerabilities, right? You know, just a genius. And he, you know, really seal is a kind of multilateral coalition type of white hat security effort that has coalesced around him. You know, A16Z has supported it. Paradigm has supported it. Delphi, you know, and myself has supported it. A lot of different people. A lot of security researchers, white hats, gray hats even, may perhaps, you know, centralized exchanges, they've all kind of been involved in this. And you know, they do a number of things. One of the things that they do really well, I think, is these tabletop exercises, but customized to the DeFi environment, right, which is different in any number of respects. And they actually, they kind of go even farther. They actually have found really great ways to actually simulate and attack, right? So deploy, you know, a forked version of, let's say, compound on a test net, you know, with a vulnerability and set things up ahead of time in terms of alerts and then trigger the alerts unexpectedly and then people have to respond, you know, and things like that. And it is, you know, I think, I'm not personally a big part of that. You know, I've been in some war rooms and stuff, you know, where people were caught off guard. And I definitely think that's a fantastic practice, you know, for people who can set it up. The other thing that Seales.net that I was much more involved in is the idea of trying to create what I would describe as a combination, social, legal, and technical protocol for white hat rescues out of smart contracts, right? So we call that the white hat safe harbor agreement. And the idea is that, you know, it really is designed for someone like Samsung, right, who might spot a vulnerability in a smart contract. And then he has to really sit there and ask himself, what should he do? What can he do? What risks is he running by virtue of what interventions, right? You know, so the very simple thing you can do, you know, which I probably recommend in, you know, 99% of cases or a lot of cases is simply find a way to contact the protocol team, you know, that's most responsible for that if there is one, and confidentially disclose this vulnerability to them. And yeah, and down at exactly, down on Twitter, you know, and then they can, you know, you can work with them to try to help mitigate it. Sometimes however, the attack may be imminent, right, or someone else may have already started an attack using that vulnerability. And or there may not be a team anymore, perhaps it's a neglected protocol. And we've actually seen some of this, right, in terms of like tornado cash is almost an example of it, right? Because, you know, the devs essentially weren't really allowed to do much with it anymore. And someone found a loan to, you know, to do an arbitrary governance token mint. And suddenly they were attacking the protocol. And guess what, there is no protocol team anymore, right? So, so, and, you know, it's not only North Korea's funds and tornado cash, it's also innocent people's right. So, so the other thing you could potentially consider doing, and I think we've seen some of this is just really try to rescue the funds. And this happened in the original the Dow hack. You know, there was the white hat sub Dow, right? And then there were like counter attacks by the Dow hacker against the white hat sub Dow's right. And, and then we, so you could, you know, if you're someone like Samsung, you could literally just try to intervene in this situation. And so what we tried to do is create a contract that protocol teams, protocol Dow's protocol user interfaces and thus by extension protocol users can adopt and agree to in advance that would kind of get you to a similar place as what pentesting and bug bounty programs might get you to in a Web 2 context where you say, okay, if there really is an imminent danger, we are giving you our consent to try to save the funds. And you can go ahead and deduct whatever it is, 5% 10% of whatever you save, send the rest to a previously designated protocol safety address, right? And we automatically grant you the release and all the nice things that you may want, right? And so, you know, it's an actual written contract together with a few smart contract tools, right, that, that can be adopted for different protocols to try to give white hats who have the capability of rescuing funds an opportunity to do so legally. Now this is a little can be controversial and a little bit of a gray area as well, because I think what you have seen in the space is a lot of black hats who turn around and then try to pretend that they're white hats, right? That's a whole song and dance. It's not intended, you know, it's not intended for that. It is intended for and that literally is extortion in most cases, right? It's someone who acts the protocol, then they send an on-chain message and they say, hey, I have the funds, let's negotiate my white hat bounty and then I'll send it back to you, right? That is not a white hat intervention, guys. Okay, that is illegal, all right? It's called extortion. So we wanted to create a legal social technical framework for clearly distinguishing the white hats from the black hats and trying to make it safe to rescue funds. So that's yet another of many, many different things that SEAL is doing. So just to ask you a question on that, because you mentioned pentests. Now in pentests, there are basically three types of pentests, white, gray and black. And they don't exactly mean the same as white hat hacker. It goes to the ability to penetrate into like, you know, do you have full access rates? Do you have sort of lower level permissioned rights where you're in the system or you're able to access the system to smart contracts, but you're administratively limited and then there's fully black hat where you're just like an external exploiter, what have you? Obviously if you actually go and get a penetration test and you do the white, white level penetration test, it's much more expensive than if you do the black. But is there any intent for SEAL to kind of get into like, move beyond the black into like the gray? Yeah, there kind of is some distinctions drawn there within the agreement. One thing that is clearly out of bounds would be an Avi Eisenberg style attack where you first manipulate a market, which is illegal in itself. It's called market manipulation. And to back up a little bit, we have anti-hacking statutes in most countries have them. In the US it's called the Computer Fraud and Abuse Act. The reason why hackers can legally do penetration testing for like a company or something is the company owns the servers, the company gives advance consent and consent actually means that it's no longer unauthorized access into a system and therefore it is legal. There's a bunch of like difficulties in applying that logic to smart contracts, but here we're just trying to preempt all that by giving consent to whatever extent it may matter. But we don't give the consent to market manipulation because number one, even if we gave the consent, that's a crime does not depend on whether someone has consented to it. It is simply a crime. You cannot cure it with consent. But the Computer Fraud stuff you can cure with consent perhaps, right? So that's kind of the distinction. Could there be scenarios where she'll get from almost like a multi-sig emergency access so they can actually do like that kind of pen test? I guess that would be more in a sandbox. Yeah, yeah, it'd be different. It wouldn't be under the white hat, safe harbor. It'd be some kind of contractual custom arrangement, more like a traditional bounty program. I mean, that would be something to be grown into. And I think like one of the things that SEAL does, and I think, you know, it's still, I mean, there's a lot of different organizations that set forth these sort of best practices standards for open source because open source is obviously a different animal than closed source, but there's a lot of similarities. For example, like one organization that has just basic things that you can do and in addition to SEAL, because it takes a number of these different things to work together, there's the open source security foundation that also does a lot in terms of providing the standards that protocols can try to abide by like in a preventative sense. Right. And so, you know, these are the like, there's many standards out there for cyber security. I think the problem is, is just, you know, most of these orgs don't have CISOs. They don't have like somebody who's centrally controlling it. It's like, you're wearing the dev. Oh, he's my head of dev. He's the head of security. In fact, it's supposed to be kind of separate because the guy who's the head of dev may not be seeing objectively some of the security flaws he's up against as he's trying to push things out. So, you know, I think that's an area of maturation for the whole space. And certainly as we look to institutionalization, which may be not always a good, like, you know, there's a different views on how much we should try to, you know, engage with institutions. I think inevitably there's going to be projects that want to engage more with institutions because that's where the money is and they're going to mature. And those organizations are going to be under pressure to develop these standards and to abide by these standards and to be able to demonstrate. I mean, it's already happening today. Like you have projects like, I was just talking with Collins about like, Ando and what they're trying to do and they're going to have to comply with all the similar standards that like a broker dealer would have to comply with. Why? Because any broker dealer has to comply with those standards. Broker dealers, you know, whether you're regulated by the FIA, Futures Industry Association or FINRA, there are vendor outsourcing guidance that you have to comply with. FINRAs is very involved, particularly around cybersecurity and the things that you have to do with your vendor and risk we, those vendors. So this is going to bleed over to the extent that we're looking at institutional adoptions for some of these things because by necessity, those institutions are going to demand that from those protocols. Then there's that green space between those that are trying to get the institutional adoption, those that are starting out that maybe if they get hacked, it's not, I don't know if it's not so bad. It's just, it's like startups, you're going to have that problem. They're immature by definition. You know, real brief, like I was, when I first started doing risk assessments, I just come from the exchange. I had like, I knew my stuff cold. I had volumes and this and that thing. The first assessment I did, the guy came back, he says, that's a little exchange-y, don't you think? And I was just like, oh, stab because I knew what I had done. I totally blown it. I had then adjusted the risk down to the maturity level of the organization I was dealing with. And so that's what's critical. You can't go to a startup and say, yeah, you've got to comply with those. It's no, it can't happen. It's never going to happen. Forget it. And they're probably not even going to listen to you. But once they start to get that uplift, that's when you've got to start doing things like, and I think like, you know, SEAL's great. It has to mature itself as an organization because it can't do everything voluntarily, right? But I think organizations like that partnering early and just trying to do something, like one other thing, I'm just going to, well, it doesn't have one other thing with me, right? But I remember a lot of times when I started dealing with younger projects and they would say, and I'm referring to RIAs and BDs, and they would say, you know, can't you just do a few things that get me up to speed and for security? It's like, no. Because it's all about really understanding the risk. You know, and if everybody who's read my tone, you know, I talk about risk-weeding and the importance of risk-weeding. And risk-weeding, you don't necessarily have to do a tone to kind of address that, but you do need to understand what risk you're trying to mitigate against. So a startup organization desperately needs sort of an understanding of the risk that they're trying to address. And it's not just something that you're guessing at. It needs to be, somebody needs to take, you know, go through like sort of a standardized list, preferably open source, preferably, you know, with the help of SEAL, I can develop these standards and just address it like how we're going to do this. And it doesn't mean you throw the kitchen sink at everything. You say, where's my biggest risk? What are the things I can depend on? And you look at it year after year after year for continuous improvement. One of the things with all these vendor outsourcing guidance that Center has that institutions are going to apply to whatever protocols they try to connect with or any organization, any type of risk management, any kind of CMMC, I don't know if you're familiar with those kind of or SOC2 or whatever. It's this on-bill and continuous improvement process. It's not a one-up down. You don't have a risk assessment or a SOC2 and good, I'm done. It's a continuous process of assessing where you've been, what mistakes you've made, and how can you improve. I tell startups, I'm like, listen, you know, you have to succeed. In your first, you set a program plan for your first year, right? We get there step by step. You set a program plan. What do I need to accomplish in the next year to make myself marginally more secure? You do that assessment and you set those goals. After that one year's time is done, you reassess where you are and you determine your next one year goal. You're not trying to have the most sophisticated, mature security setup. It's never going to happen. Forget it. Start where you are and plan into it. Anyway, you can tell I'm a little passionate about that. But it makes a lot of sense. Two other super quick things I would highlight just to put on people's radar. One is circles proposed reversibility, reversible token protocol. That's meant to address this stuff. It's obviously controversial. I have mixed thoughts on it myself, but I think since it's been proposed by Circle and the original papers by Dan Bonet and some other gigabrains, it's definitely something to have on your radar and think about in terms of these issues. The other issue I would say is, oh, I had something here, but I lost it. Just keep that on your radar. I would just generally say in terms of like you really have to think about the day, you know, Eric's talking about a lot of broker dealer like traditional requirements, but you really always have to think about the difference between web two and web three. One of the big differences with web three is that no one owns this smart contract. It is an autonomous piece of code. It's essentially public infrastructure. There isn't some one party who can say, yes, I accept that you will do some run some test on this or whatever. Unless you have a board. When these teams, a lot of times when there is a hack, these development teams jump in and they start negotiating with this, whether they're describing themselves as a white hat or a black hat or whatever, they start negotiating. They're saying, yeah, you can, if you give 10%, if you keep 10% and give the rest back, we'll give you a release. Well, guess what? It's not your money, guys. Like you don't have the ability, you actually are not allowed to get, you don't have any authorization to give a release for that because it's the user's money, right? And so you need to start thinking about ways. How can you disaster plan at the beginning? Can you have something in the terms of service for the web apps associated with these protocols where the users actually authorize the development team to negotiate this particular issue on their behalf and offer, say, up to 10% bounty or something like that. There really needs to be way more thought than that. The wallets could help a lot with that with certain UX. A lot of, you know, there's just a lot of things, but you really don't just simple mindedly take web 2 security recommendations and think you can transpose them. This is a very different context. Any questions from the audience? Any for Dave? I'm the new FTC, these five of them are closure public companies within four days. You guys have any, like, how that's going to affect public companies disclosure or cyber security? I haven't gone through those proposals in depth. One of the concerns that I've always had with the nature of those disclosures is sometimes those disclosures sort of reveal the vulnerabilities themselves. You have to be really careful about that. So there's a balance between trying to ensure that there's sufficient information to the public and not giving hackers potentially more information to exploit. The FBI and the Crescent view 10 points to extend that 48 limit. What is the material? I actually don't advise any public companies on cyber attacks, but again, if it's too much information too early, it can impede investigations, it can impede efforts to correct. The question is, there's transparency, but really at the end of the day, you want to make sure that all the companies or any company that's trusted with the investor's money is taking the appropriate steps, whether it's fully disclosed or not. There's really just an element of accountability. So I can't say that I'm in favor of that. To me, I think there's other ways to accomplish the same thing less problematically. I'm not necessarily against it, but I like to see it as a more administrative state. Go figure. Other questions? Thanks. A common concern I hear from Dow founders is that someone will buy a bunch of their governance tokens really fast and then perform a governance attack. How significant is that risk, especially when there isn't often enough liquidity available to buy that many tokens without anybody noticing? Is it a valid concern or is it overblown? Yeah, I think it's a valid concern. Well, it depends. The interesting thing here is that the more fake your Dow is, the less that particular attack matters. If your Dow is all snapshot voting, then it really doesn't matter because snapshot voting doesn't have any power. You'll note that AVE, which is obviously one of the best protocols from some of the best people, blah, blah, blah. They have a guardian multi-sig that can actually overrule any Dow governance proposal. That's how concerned they are about it. There were, back in the day, MakerDow had people talked about a MakerDow vulnerability related to flash loans of MKR and stuff like that. The more power the Dow has, AVE has a lot of power. I think that's why they're concerned about it. The more of a real attack vector this is, and I will say with some of the Borg stuff that I was talking about earlier, now we are giving Dow's not full power over any, but they certainly have certain powers. You also have to think about governance attacks in that context. You certainly need an escape valve. Anything that the Dow could do that would actually be illegal for the entity to do or break the entity's rules. You have to design where that's not possible because you have to assume that absolutely anyone could get control over these governance tokens. It's really a lot like the public company, Activist Shareholder context. We realize that management of companies can suck, become stagnant. We want to allow people like Coral Icon to come in and buy up a bunch of stock and say, I can do this better. I'm kicking you guys out and I'm going to fix this company. We want to allow that. On the other hand, there are predatory versions of that known as Green Mail where the evil version of Coral Icon, who by the way was just Coral Icon of the 80s, comes in and says, and just says, hey fuckers, you're all getting fired unless you just declare like a huge dividend right now. That goes back to that's essentially extortionate. It's a very fine balance that I think most people in crypto do not come from a public companies context. People who I think are smartest about this stuff are ARCA. They actually kind of style themselves as like Dow activist investors and have had some successful campaigns. But that really is the dynamic and it's something that we're all going to learn and navigate. Probably just one more question. No worries. Hey, so I kind of have a tangent question because you mentioned Coral Icon. The next five to 10 years of the space, I really see like right now there's a lot of mimicry in the space. A lot of things that do essentially the exact same things. Obviously, there's going to be like a lot of M&A activity over the next five to 10 years. I just wanted to hear your thoughts on it and like how do you think the industry is going to consolidate especially across chains? Yeah, I mean as a former M&A lawyer, I've always been very interested in this idea of protocol murders and how you could do them and whether they would ever make sense. And I think now we've kind of seen some attempted. I think there was Rary was one. There are a few others. I think they were done from a legal perspective very, very sloppily. But one of the things that I think like people could actually build this type of thing like as a function into their protocol or there could be some meta governance protocol that facilitates it. You could imagine some sort of like trustless way of launching like a tender offer, like one Dow launching a tender offer for another Dow's tokens. And if and only if the like 90% of tokens are tendered, right, then there would have been something already built into the other protocol that just forces everyone else to go along with it. It's like a super majority approval almost. But you need that. You need to surely do M&A. We need to have mechanisms like that because merger statutes are essentially minority squeeze out mechanisms, right? Where if you basically get sufficient shareholders agreeing, then guess what? Even if that last 10%, doesn't want to do the merger, they're forced, right? So there would need to be to truly do it. There needs to be something like that. That's one of the things I'd kind of like to help build either at Medelex or in some other context, get people thinking about it. But keep in mind, we have a bunch of legal stuff around this as well, right? Where there's something called a creeping, a creeping hostile acquisition in the public company context. There was all the read about the 80s M&A wave, you know, barbarians at the gate, all these great books and movies about it. But some of the ways that the predatory aspects of that were stopped were securities laws were all in there, running eight securities all over those. But where if you acquire, if someone requires 5% of the float of a share, they have to disclose that they've acquired it, right? Either alone or with a group, right? And what are their intentions with this, right? That way the company's management knows, the other shareholders know, oh, there's someone who's accumulating a lot of stock and it might put them on notice because what you don't want to do is let everyone, there's something called a control premium in corporate finance, right? Where it's so much more valuable to own 51% of a company than 49% because you can get to pick the directors and choose the strategy and stuff that you need to pay a premium to get from 49% to 51%. If you're able to acquire a company in secret, you can avoid paying that premium. So we have all kinds of legal mechanisms to avoid people doing that trick and you would need similar type of shit at the protocol layer to really make protocol M&A a robust thing. Thank you. All right. Thank you very much Eric Hess. Thank you, Gid. Thank you all. Thank you. Thank you all.